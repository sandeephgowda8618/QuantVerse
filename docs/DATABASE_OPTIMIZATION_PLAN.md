# Alpha Vantage Database Performance Optimization Plan

## ðŸ“Š Current Database Architecture Analysis

### âœ… **What's Already Good**
1. **Proper Schema Design**: Well-structured tables for different data types
2. **Connection Pooling**: Both sync and async connection pools
3. **Conflict Resolution**: UPSERT operations for idempotency
4. **Indexes**: Basic indexes on ticker, timestamp, endpoint
5. **JSONB Storage**: Flexible storage for varying API responses

### ðŸš€ **Performance Improvement Opportunities**

## 1. **Enhanced Indexing Strategy**

### Current Issues:
- Missing compound indexes for common query patterns
- No partial indexes for filtered queries
- No expression indexes for JSONB queries

### Solutions:
```sql
-- Compound indexes for time-series queries
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_alpha_market_ticker_time_endpoint 
ON alpha_market_data(ticker, timestamp DESC, endpoint) 
INCLUDE (open_price, high_price, low_price, close_price, volume);

-- Partial indexes for active/recent data
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_alpha_market_recent_90d
ON alpha_market_data(ticker, timestamp DESC) 
WHERE timestamp > (NOW() - INTERVAL '90 days');

-- JSONB expression indexes for common queries
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_alpha_fund_market_cap_gin
ON alpha_fundamental_data USING gin ((financial_data->>'MarketCap')) 
WHERE data_type = 'overview';

-- Covering indexes to avoid table lookups
CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_alpha_market_price_summary
ON alpha_market_data(ticker, timestamp DESC) 
INCLUDE (close_price, volume, quality_flag)
WHERE api_function = 'TIME_SERIES_DAILY';
```

## 2. **Table Partitioning for Time-Series Data**

### Problem:
Large tables slow down queries as data grows

### Solution:
```sql
-- Partition market data by month
CREATE TABLE alpha_market_data_template (
    LIKE alpha_market_data INCLUDING ALL
) PARTITION BY RANGE (timestamp);

-- Create monthly partitions
CREATE TABLE alpha_market_data_2024_11 PARTITION OF alpha_market_data_template
FOR VALUES FROM ('2024-11-01') TO ('2024-12-01');

-- Auto-partition creation function
CREATE OR REPLACE FUNCTION create_monthly_partition(table_name text, start_date date)
RETURNS void AS $$
DECLARE
    partition_name text;
    end_date date;
BEGIN
    partition_name := table_name || '_' || to_char(start_date, 'YYYY_MM');
    end_date := start_date + interval '1 month';
    
    EXECUTE format('CREATE TABLE IF NOT EXISTS %I PARTITION OF %I FOR VALUES FROM (%L) TO (%L)',
                   partition_name, table_name, start_date, end_date);
END;
$$ LANGUAGE plpgsql;
```

## 3. **Batch Insert Optimization**

### Current Issues:
- Single row inserts are slow
- No use of COPY for bulk operations

### Solutions:
```sql
-- Use COPY for maximum performance
COPY alpha_market_data(ticker, timestamp, open_price, high_price, low_price, close_price, volume, raw_payload, ingestion_epoch)
FROM STDIN WITH (FORMAT csv, HEADER false);

-- Batch UPSERT with VALUES clause (faster than executemany)
INSERT INTO alpha_market_data(ticker, timestamp, endpoint, ...)
VALUES 
    ('AAPL', '2024-11-09 10:00:00', 'GLOBAL_QUOTE', ...),
    ('MSFT', '2024-11-09 10:00:00', 'GLOBAL_QUOTE', ...),
    -- ... 1000 rows at once
ON CONFLICT (ticker, timestamp, endpoint) DO UPDATE SET
    raw_payload = EXCLUDED.raw_payload,
    updated_at = NOW();
```

## 4. **Memory and Performance Settings**

### PostgreSQL Configuration:
```sql
-- Optimize for write-heavy workload
shared_buffers = 25% of RAM
work_mem = 256MB  
maintenance_work_mem = 1GB
wal_buffers = 16MB
checkpoint_timeout = 15min
max_wal_size = 4GB
effective_cache_size = 75% of RAM

-- For Alpha Vantage ingestion specifically
temp_buffers = 128MB
max_connections = 100
```

## 5. **Query Optimization Patterns**

### Common Query Patterns:
```sql
-- âœ… GOOD: Use indexes efficiently
SELECT ticker, close_price, volume
FROM alpha_market_data 
WHERE ticker = $1 
  AND timestamp >= $2 
  AND api_function = 'TIME_SERIES_DAILY'
ORDER BY timestamp DESC 
LIMIT 100;

-- âŒ BAD: Full table scan
SELECT * FROM alpha_market_data 
WHERE raw_payload::text LIKE '%AAPL%';

-- âœ… GOOD: JSONB optimization
SELECT ticker, financial_data->>'MarketCap' as market_cap
FROM alpha_fundamental_data 
WHERE data_type = 'overview'
  AND (financial_data->>'MarketCap')::numeric > 1000000000;
```

## 6. **Connection Pool Optimization**

### Current Issues:
- Fixed pool sizes may not match workload
- No monitoring of pool utilization

### Solutions:
```python
# Dynamic pool sizing based on workload
async def optimize_pool_size():
    """Adjust pool size based on ingestion load"""
    active_tickers = len(get_active_ingestion_tickers())
    optimal_size = min(max(active_tickers // 4, 5), 50)
    
    if optimal_size != current_pool_size:
        await recreate_pool(optimal_size)

# Connection pool monitoring
class OptimizedPostgresHandler(PostgresHandler):
    def __init__(self):
        super().__init__()
        self.pool_stats = {
            'total_connections': 0,
            'active_connections': 0,
            'wait_time_avg': 0.0
        }
    
    async def get_pool_stats(self):
        """Monitor pool performance"""
        if self.async_pool:
            return {
                'size': self.async_pool.get_size(),
                'idle': self.async_pool.get_idle_size(),
                'busy': self.async_pool.get_size() - self.async_pool.get_idle_size()
            }
```

## 7. **Data Compression and Storage**

### JSONB Compression:
```sql
-- Enable compression for large JSONB columns
ALTER TABLE alpha_market_data ALTER COLUMN raw_payload SET STORAGE EXTENDED;
ALTER TABLE alpha_fundamental_data ALTER COLUMN financial_data SET STORAGE EXTENDED;

-- Archive old data to compressed tables
CREATE TABLE alpha_market_data_archive (
    LIKE alpha_market_data
) WITH (fillfactor=100);  -- No updates, optimize for storage
```

## 8. **Monitoring and Alerting**

### Performance Monitoring:
```sql
-- Query to find slow operations
SELECT 
    schemaname,
    tablename,
    n_tup_ins as inserts,
    n_tup_upd as updates,
    n_tup_del as deletes,
    n_tup_hot_upd as hot_updates
FROM pg_stat_user_tables 
WHERE tablename LIKE 'alpha_%'
ORDER BY n_tup_ins DESC;

-- Index usage analysis
SELECT 
    schemaname,
    tablename,
    indexname,
    idx_tup_read,
    idx_tup_fetch,
    idx_scan
FROM pg_stat_user_indexes 
WHERE tablename LIKE 'alpha_%'
ORDER BY idx_scan DESC;
```

## 9. **Estimated Performance Gains**

| Optimization | Current Time | Optimized Time | Improvement |
|--------------|--------------|----------------|-------------|
| Single Insert | 5ms | 0.5ms | **90% faster** |
| Batch Insert (1000) | 5000ms | 100ms | **98% faster** |
| Time-series Query | 200ms | 10ms | **95% faster** |
| JSONB Search | 1000ms | 50ms | **95% faster** |
| Daily Ingestion | 8 hours | 1 hour | **87% faster** |

## 10. **Implementation Priority**

### Phase 1 (Immediate - 1 day):
1. âœ… Enhanced indexes
2. âœ… Batch insert optimization
3. âœ… Connection pool tuning

### Phase 2 (Short-term - 1 week):
1. âœ… Table partitioning
2. âœ… PostgreSQL configuration tuning
3. âœ… Query optimization

### Phase 3 (Long-term - 1 month):
1. âœ… Data archiving strategy
2. âœ… Advanced monitoring
3. âœ… Auto-scaling pools

## ðŸ’¡ **Immediate Actions for Alpha Vantage Pipeline**

1. **Create optimized indexes** before next ingestion run
2. **Implement batch writing** in AlphaWriter
3. **Tune PostgreSQL settings** for write-heavy workload
4. **Add connection pool monitoring**
5. **Enable query logging** to identify bottlenecks

This optimization plan should **reduce ingestion time by 80-90%** and handle the full 2,025 daily requests efficiently.
