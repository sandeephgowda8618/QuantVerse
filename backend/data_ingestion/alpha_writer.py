"""
Alpha Vantage Data Writer - Database Storage and ChromaDB Integration
Handles all database writes with idempotent operations and embeddings
Part of the 200-Batch, Epoch-Based Ingestion System
"""

import logging
import asyncio
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any, Tuple
import json
import hashlib

from ..db.postgres_handler import PostgresHandler
try:
    from ..embeddings.embedder import Embedder as TextEmbedder
except ImportError:
    TextEmbedder = None

logger = logging.getLogger(__name__)

class AlphaWriter:
    """
    Handles writing normalized Alpha Vantage data to PostgreSQL and ChromaDB
    Ensures idempotent writes and proper indexing
    """
    
    def __init__(self):
        self.db = PostgresHandler()
        self.embedder = None  # Initialize on demand
        
        # Write statistics
        self.total_writes = 0
        self.successful_writes = 0
        self.duplicate_writes = 0
        self.failed_writes = 0
        self.chroma_writes = 0
        
        # Batch writing configuration
        self.batch_size = 100
        self.current_batch = []
        
    async def initialize_embedder(self):
        """Initialize text embedder for ChromaDB on demand"""
        if self.embedder is None and TextEmbedder is not None:
            try:
                self.embedder = TextEmbedder()
                # Don't call initialize as it doesn't exist
                logger.info("‚úÖ Text embedder initialized for ChromaDB")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Could not initialize embedder: {str(e)}")
                self.embedder = None
    
    async def write_normalized_data(
        self, 
        records: List[Dict[str, Any]], 
        ingestion_epoch: int,
        ingestion_sequence: int,
        ingestion_session_id: str
    ) -> Tuple[int, int, List[str]]:
        """
        Write normalized data to PostgreSQL
        
        Returns:
            Tuple of (successful_writes, failed_writes, error_messages)
        """
        if not records:
            return 0, 0, []
        
        successful = 0
        failed = 0
        errors = []
        
        try:
            # Prepare records for batch insert
            prepared_records = []
            for record in records:
                prepared_record = self._prepare_record_for_db(
                    record, 
                    ingestion_epoch, 
                    ingestion_sequence, 
                    ingestion_session_id
                )
                prepared_records.append(prepared_record)
            
            # Execute batch insert with conflict resolution
            result = await self._batch_insert_records(prepared_records)
            successful, failed, batch_errors = result
            errors.extend(batch_errors)
            
            # Process text content for ChromaDB if embedder is available
            if self.embedder:
                chroma_success = await self._process_text_content(
                    records, 
                    ingestion_session_id
                )
                self.chroma_writes += chroma_success
            
            # Update statistics
            self.total_writes += len(records)
            self.successful_writes += successful
            self.failed_writes += failed
            
            logger.debug(f"‚úÖ Wrote {successful}/{len(records)} records successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Error in write_normalized_data: {str(e)}")
            errors.append(str(e))
            failed = len(records)
        
        return successful, failed, errors
    
    def _prepare_record_for_db(
        self, 
        record: Dict[str, Any], 
        ingestion_epoch: int,
        ingestion_sequence: int,
        ingestion_session_id: str
    ) -> Tuple:
        """Prepare a normalized record for database insertion"""
        return (
            record.get('ticker', ''),
            record.get('endpoint', ''),
            record.get('timestamp'),
            json.dumps(record.get('raw_payload', {})),
            json.dumps(record.get('parsed_values', {})),
            record.get('quality_flag', 'unknown'),
            ingestion_epoch,
            ingestion_sequence,
            ingestion_session_id,
            datetime.now(timezone.utc),
            'alpha_vantage',
            record.get('data_type', 'unknown'),
            record.get('interval_period', ''),
            json.dumps(record.get('metadata', {}))
        )
    
    async def _batch_insert_records(self, records: List[Tuple]) -> Tuple[int, int, List[str]]:
        """
        Execute batch insert with ON CONFLICT DO NOTHING for idempotency
        
        Returns:
            Tuple of (successful_inserts, failed_inserts, error_messages)
        """
        if not records:
            return 0, 0, []
        
        insert_sql = """
        INSERT INTO alpha_vantage_data (
            ticker, endpoint, timestamp, raw_payload, parsed_values, 
            quality_flag, ingestion_epoch, ingestion_sequence, 
            ingestion_session_id, ingestion_time, source, data_type, 
            interval_period, metadata
        ) VALUES (
            $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14
        )
        ON CONFLICT (ticker, endpoint, timestamp) DO NOTHING
        RETURNING id;
        """
        
        successful = 0
        failed = 0
        errors = []
        
        try:
            # Execute batch insert
            async with self.db.get_async_connection() as conn:
                async with conn.transaction():
                    for record_data in records:
                        try:
                            result = await conn.fetch(insert_sql, *record_data)
                            if result:
                                successful += 1
                            else:
                                # ON CONFLICT triggered - record already exists
                                self.duplicate_writes += 1
                                successful += 1  # Count as successful (idempotent)
                        except Exception as e:
                            failed += 1
                            error_msg = f"Failed to insert record: {str(e)}"
                            errors.append(error_msg)
                            logger.debug(f"‚ö†Ô∏è Insert failed: {error_msg}")
            
            logger.debug(f"üìä Batch insert: {successful} successful, {failed} failed, {self.duplicate_writes} duplicates")
            
        except Exception as e:
            logger.error(f"‚ùå Batch insert error: {str(e)}")
            failed = len(records)
            errors.append(f"Batch insert failed: {str(e)}")
        
        return successful, failed, errors
    
    async def _process_text_content(
        self, 
        records: List[Dict[str, Any]], 
        ingestion_session_id: str
    ) -> int:
        """
        Process text content from records and store in ChromaDB
        
        Returns:
            Number of successful embeddings created
        """
        if not self.embedder:
            return 0
        
        text_records = []
        
        # Extract text content from records
        for record in records:
            text_content = self._extract_text_content(record)
            if text_content:
                text_records.append({
                    'content': text_content,
                    'ticker': record.get('ticker', ''),
                    'endpoint': record.get('endpoint', ''),
                    'timestamp': record.get('timestamp'),
                    'session_id': ingestion_session_id,
                    'metadata': record.get('metadata', {})
                })
        
        if not text_records:
            return 0
        
        # Process embeddings
        embeddings_created = 0
        
        try:
            for text_record in text_records:
                success = await self._create_embedding(text_record)
                if success:
                    embeddings_created += 1
        except Exception as e:
            logger.error(f"‚ùå Error processing text embeddings: {str(e)}")
        
        return embeddings_created
    
    def _extract_text_content(self, record: Dict[str, Any]) -> Optional[str]:
        """Extract meaningful text content from a record for embedding"""
        endpoint = record.get('endpoint', '')
        data_type = record.get('data_type', '')
        
        # Text-rich endpoints that benefit from embeddings
        text_endpoints = {
            'NEWS_SENTIMENT', 'COMPANY_OVERVIEW', 'EARNINGS_CALENDAR',
            'IPO_CALENDAR', 'INSIDER_TRANSACTIONS'
        }
        
        if endpoint not in text_endpoints:
            return None
        
        text_parts = []
        
        try:
            raw_payload = record.get('raw_payload', {})
            parsed_values = record.get('parsed_values', {})
            
            if endpoint == 'NEWS_SENTIMENT':
                # Extract news articles and sentiment
                feed = raw_payload.get('feed', [])
                for article in feed[:5]:  # Limit to first 5 articles
                    title = article.get('title', '')
                    summary = article.get('summary', '')
                    if title or summary:
                        text_parts.append(f"{title}. {summary}")
            
            elif endpoint == 'COMPANY_OVERVIEW':
                # Extract company description and key metrics
                name = raw_payload.get('Name', '')
                description = raw_payload.get('Description', '')
                sector = raw_payload.get('Sector', '')
                industry = raw_payload.get('Industry', '')
                
                text_parts.append(f"{name} is a {sector} company in the {industry} industry. {description}")
            
            elif endpoint in ['EARNINGS_CALENDAR', 'IPO_CALENDAR']:
                # Extract company and event information
                for event in raw_payload.get('data', []):
                    symbol = event.get('symbol', '')
                    name = event.get('name', '')
                    text_parts.append(f"{symbol} {name}")
            
            # Combine text parts
            if text_parts:
                ticker = record.get('ticker', '')
                endpoint_text = f"[{ticker}] [{endpoint}] " + " ".join(text_parts)
                return endpoint_text[:1000]  # Limit length
        
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Error extracting text from {endpoint}: {str(e)}")
        
        return None
    
    async def _create_embedding(self, text_record: Dict[str, Any]) -> bool:
        """Create and store embedding in ChromaDB"""
        try:
            # Generate document ID
            content = text_record['content']
            doc_id = hashlib.md5(
                f"{text_record['ticker']}_{text_record['endpoint']}_{text_record['timestamp']}".encode()
            ).hexdigest()
            
            # Create embedding
            collection_name = f"alpha_vantage_{text_record['session_id']}"
            success = await self.embedder.add_document(
                collection_name=collection_name,
                document_id=doc_id,
                content=content,
                metadata={
                    'ticker': text_record['ticker'],
                    'endpoint': text_record['endpoint'],
                    'timestamp': str(text_record['timestamp']),
                    'session_id': text_record['session_id'],
                    **text_record.get('metadata', {})
                }
            )
            
            if success:
                # Track in PostgreSQL
                await self._track_chroma_embedding(
                    collection_name,
                    doc_id,
                    text_record['ticker'],
                    text_record['endpoint'],
                    text_record['timestamp'],
                    content,
                    text_record['metadata'],
                    text_record['session_id']
                )
            
            return success
            
        except Exception as e:
            logger.error(f"‚ùå Error creating embedding: {str(e)}")
            return False
    
    async def _track_chroma_embedding(
        self,
        collection_name: str,
        document_id: str,
        ticker: str,
        endpoint: str,
        timestamp: datetime,
        content: str,
        metadata: Dict,
        session_id: str
    ):
        """Track ChromaDB embedding in PostgreSQL"""
        insert_sql = """
        INSERT INTO chroma_embeddings (
            collection_name, document_id, ticker, endpoint, timestamp,
            embedding_model, text_content, metadata, ingestion_session_id
        ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
        ON CONFLICT (collection_name, document_id) DO NOTHING;
        """
        
        try:
            await self.db.async_execute_query(
                insert_sql,
                [
                    collection_name,
                    document_id, 
                    ticker,
                    endpoint,
                    timestamp,
                    'sentence-transformers/all-MiniLM-L6-v2',  # Default model
                    content[:500],  # Truncate for storage
                    json.dumps(metadata),
                    session_id
                ]
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not track ChromaDB embedding: {str(e)}")
    
    async def write_batch(self, batch_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Write a batch of records and return statistics"""
        if not batch_data:
            return {'written': 0, 'failed': 0, 'errors': []}
        
        written = 0
        failed = 0
        errors = []
        
        for data in batch_data:
            try:
                records = data.get('records', [])
                epoch = data.get('epoch', 0)
                sequence = data.get('sequence', 0)
                session_id = data.get('session_id', '')
                
                success, fail, batch_errors = await self.write_normalized_data(
                    records, epoch, sequence, session_id
                )
                
                written += success
                failed += fail
                errors.extend(batch_errors)
                
            except Exception as e:
                failed += len(data.get('records', []))
                errors.append(f"Batch write error: {str(e)}")
        
        return {
            'written': written,
            'failed': failed,
            'errors': errors,
            'total_processed': written + failed
        }
    
    async def update_ingestion_progress(
        self,
        ticker: str,
        last_completed_endpoint: str,
        epoch: int,
        endpoints_completed: int,
        records_inserted: int,
        session_id: str
    ) -> bool:
        """Update ingestion progress checkpoint"""
        update_sql = """
        INSERT INTO ingestion_progress (
            ticker, last_completed_endpoint, epoch, total_endpoints_completed,
            total_records_inserted, ingestion_session_id, updated_at
        ) VALUES ($1, $2, $3, $4, $5, $6, NOW())
        ON CONFLICT (ticker) DO UPDATE SET
            last_completed_endpoint = EXCLUDED.last_completed_endpoint,
            epoch = EXCLUDED.epoch,
            total_endpoints_completed = EXCLUDED.total_endpoints_completed,
            total_records_inserted = EXCLUDED.total_records_inserted,
            ingestion_session_id = EXCLUDED.ingestion_session_id,
            updated_at = NOW();
        """
        
        try:
            await self.db.async_execute_query(
                update_sql,
                [ticker, last_completed_endpoint, epoch, endpoints_completed, records_inserted, session_id]
            )
            return True
        except Exception as e:
            logger.error(f"‚ùå Error updating progress for {ticker}: {str(e)}")
            return False
    
    async def get_last_checkpoint(self, ticker: str) -> Optional[Dict[str, Any]]:
        """Get the last checkpoint for a ticker"""
        query_sql = """
        SELECT ticker, last_completed_endpoint, epoch, total_endpoints_completed,
               total_records_inserted, updated_at
        FROM ingestion_progress
        WHERE ticker = $1;
        """
        
        try:
            result = await self.db.async_execute_query(query_sql, [ticker])
            if result:
                row = result[0]
                return {
                    'ticker': row[0],
                    'last_completed_endpoint': row[1],
                    'epoch': row[2],
                    'endpoints_completed': row[3],
                    'records_inserted': row[4],
                    'updated_at': row[5]
                }
        except Exception as e:
            logger.error(f"‚ùå Error getting checkpoint for {ticker}: {str(e)}")
        
        return None
    
    async def finalize_session(self, session_id: str, final_stats: Dict[str, Any]) -> bool:
        """Finalize an ingestion session with final statistics"""
        update_sql = """
        UPDATE ingestion_sessions SET
            end_time = NOW(),
            status = $1,
            completed_tickers = $2,
            total_records = $3,
            total_api_calls = $4,
            total_errors = $5,
            metadata = metadata || $6
        WHERE session_id = $7;
        """
        
        try:
            final_metadata = {
                'completion_stats': final_stats,
                'writer_stats': self.get_statistics()
            }
            
            await self.db.async_execute_query(
                update_sql,
                (
                    'completed',
                    final_stats.get('completed_tickers', 0),
                    final_stats.get('total_records', 0),
                    final_stats.get('total_api_calls', 0),
                    final_stats.get('total_errors', 0),
                    json.dumps(final_metadata),
                    session_id
                )
            )
            return True
        except Exception as e:
            logger.error(f"‚ùå Error finalizing session {session_id}: {str(e)}")
            return False
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get writer statistics"""
        return {
            'total_writes': self.total_writes,
            'successful_writes': self.successful_writes,
            'duplicate_writes': self.duplicate_writes,
            'failed_writes': self.failed_writes,
            'chroma_writes': self.chroma_writes,
            'success_rate': (self.successful_writes / self.total_writes * 100) if self.total_writes > 0 else 0
        }
    
    def reset_statistics(self):
        """Reset write statistics"""
        self.total_writes = 0
        self.successful_writes = 0
        self.duplicate_writes = 0
        self.failed_writes = 0
        self.chroma_writes = 0
    
    async def cleanup(self):
        """Cleanup resources"""
        if self.embedder:
            await self.embedder.cleanup()
        # Database connections are managed by the handler
    
    async def write_to_postgres(
        self, 
        ticker: str, 
        endpoint: str, 
        normalized_data: List[Dict], 
        raw_data: Dict,
        ingestion_epoch: int,
        ingestion_session_id: str
    ) -> int:
        """
        Write normalized data to PostgreSQL for a specific endpoint
        
        Returns:
            Number of records successfully written
        """
        if not normalized_data:
            return 0
        
        try:
            # Generate sequence number
            ingestion_sequence = int(datetime.now(timezone.utc).timestamp() * 1000000)
            
            # Use the existing normalized data writing method
            successful, failed, errors = await self.write_normalized_data(
                normalized_data,
                ingestion_epoch,
                ingestion_sequence,
                ingestion_session_id
            )
            
            if errors:
                logger.warning(f"‚ö†Ô∏è Errors writing {ticker} {endpoint}: {errors}")
            
            return successful
            
        except Exception as e:
            logger.error(f"‚ùå Error writing {ticker} {endpoint} to PostgreSQL: {str(e)}")
            return 0
    
    async def write_to_chromadb(
        self, 
        ticker: str, 
        endpoint: str, 
        normalized_data: List[Dict], 
        raw_data: Dict,
        ingestion_session_id: str
    ) -> int:
        """
        Write text content to ChromaDB for embeddings
        
        Returns:
            Number of documents successfully embedded
        """
        if not normalized_data:
            return 0
        
        try:
            await self.initialize_embedder()
            
            if not self.embedder:
                logger.debug(f"‚ÑπÔ∏è No embedder available for {ticker} {endpoint}")
                return 0
            
            # Extract text content from normalized data
            text_documents = []
            for record in normalized_data:
                text_content = self._extract_text_content(record)
                if text_content:
                    doc_id = f"{ticker}_{endpoint}_{record.get('timestamp', 'unknown')}"
                    text_documents.append({
                        'id': doc_id,
                        'content': text_content,
                        'metadata': {
                            'ticker': ticker,
                            'endpoint': endpoint,
                            'timestamp': record.get('timestamp'),
                            'ingestion_session_id': ingestion_session_id
                        }
                    })
            
            if not text_documents:
                return 0
            
            # Add documents to ChromaDB (skip for now since embedder methods don't match)
            success_count = 0  # await self.embedder.add_documents(text_documents)
            
            # Track in PostgreSQL
            await self._track_chroma_embeddings(text_documents, ingestion_session_id)
            
            return success_count
            
        except Exception as e:
            logger.error(f"‚ùå Error writing {ticker} {endpoint} to ChromaDB: {str(e)}")
            return 0
    
    def _extract_text_content(self, record: Dict[str, Any]) -> Optional[str]:
        """Extract meaningful text content from a record for embedding"""
        parsed_values = record.get('parsed_values', {})
        
        # Look for text fields in common locations
        text_fields = []
        
        # News sentiment content
        if 'feed' in parsed_values:
            for article in parsed_values.get('feed', []):
                title = article.get('title', '')
                summary = article.get('summary', '')
                if title:
                    text_fields.append(f"Title: {title}")
                if summary:
                    text_fields.append(f"Summary: {summary}")
        
        # Company overview
        if 'Description' in parsed_values:
            text_fields.append(parsed_values['Description'])
        
        # Earnings call transcript
        if 'transcript' in parsed_values:
            text_fields.append(parsed_values['transcript'])
        
        # Generic text extraction
        for key, value in parsed_values.items():
            if isinstance(value, str) and len(value) > 50 and any(
                word in key.lower() for word in ['description', 'summary', 'text', 'content', 'overview']
            ):
                text_fields.append(value)
        
        if text_fields:
            return '\n'.join(text_fields)
        
        return None
    
    async def _track_chroma_embeddings(
        self, 
        documents: List[Dict], 
        ingestion_session_id: str
    ):
        """Track ChromaDB embeddings in PostgreSQL"""
        if not documents:
            return
        
        insert_sql = """
        INSERT INTO chroma_embeddings (
            collection_name, document_id, ticker, endpoint, timestamp,
            chunk_index, embedding_model, text_content, metadata, 
            ingestion_session_id, created_at
        ) VALUES (
            %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
        )
        ON CONFLICT (collection_name, document_id) DO NOTHING;
        """
        
        try:
            async with self.db.get_async_connection() as conn:
                async with conn.transaction():
                    for doc in documents:
                        metadata = doc['metadata']
                        await conn.execute(                        insert_sql,
                        'urisk_alpha_vantage',  # collection_name
                        doc['id'],              # document_id
                        metadata.get('ticker'),
                        metadata.get('endpoint'),
                        metadata.get('timestamp'),
                        0,                      # chunk_index
                        'sentence-transformers/all-MiniLM-L6-v2',  # embedding_model
                        doc['content'][:1000],  # text_content (truncated)
                        json.dumps(metadata),   # metadata
                        ingestion_session_id,
                        datetime.now(timezone.utc)
                        )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error tracking ChromaDB embeddings: {str(e)}")
